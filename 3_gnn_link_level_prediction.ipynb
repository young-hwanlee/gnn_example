{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbS3ieiU8kTEpvbAC6hHvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young-hwanlee/gnn_examples/blob/main/3_gnn_link_level_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78jdlXgDHYTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e74de10-619d-4790-8157-47a0f5603f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gnn_examples'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 104 (delta 59), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (104/104), 3.01 MiB | 9.66 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/young-hwanlee/gnn_examples.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## To avoid some errors (출처: https://life-is-potatoo.tistory.com/95 [삶은 감자:티스토리]) ==========\n",
        "# ## 1st method\n",
        "# import torch\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# def format_pytorch_version(version):\n",
        "#     return version.split('+')[0]\n",
        "\n",
        "# TORCH_version = torch.__version__\n",
        "# TORCH = format_pytorch_version(TORCH_version)\n",
        "# def format_cuda_version(version):\n",
        "#     return 'cu' + version.replace('.', '')\n",
        "\n",
        "# CUDA_version = torch.version.cuda\n",
        "# CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "## Ensure that the PyTorch and the PyG are the same version\n",
        "# !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-geometric\n",
        "\n",
        "## 2nd method\n",
        "import os\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install torch_geometric\n",
        "\n",
        "## Ensure that the PyTorch and the PyG are the same version\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install pytorch_metric_learning\n",
        "\n",
        "## Both methods are too slow."
      ],
      "metadata": {
        "id": "ESEnouMPHe5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866cb11e-e885-4799-bab5-555d4e8f0e86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytorch_metric_learning in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_metric_learning) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch_metric_learning) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_metric_learning) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_metric_learning) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch_metric_learning) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch_metric_learning) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch_metric_learning) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch_metric_learning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch_metric_learning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import required packages ====================================================================\n",
        "# import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.nn import InnerProductDecoder\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "sEIwi5JJHe1j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up the seed for reproducibility ============================================================\n",
        "seed = 42\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)     # if use multi-GPU\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "lbxyUPYCZZ-a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a dataset =================================================================================\n",
        "from torch_geometric.datasets import Reddit\n",
        "\n",
        "dataset = Reddit(root=\"/tmp/Reddit\")"
      ],
      "metadata": {
        "id": "NOsrsmtOHeyG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Dataset: {dataset}:')\n",
        "print('============================================================================================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0].to(device)\n",
        "print(data)       # Get the 1st graph object\n",
        "print(data.x[0, :])"
      ],
      "metadata": {
        "id": "_RWS5uKQHevP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbcf9c36-53ff-4551-d999-ace55c685b6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Reddit():\n",
            "============================================================================================\n",
            "Number of graphs: 1\n",
            "Number of features: 602\n",
            "Number of classes: 41\n",
            "Data(x=[232965, 602], edge_index=[2, 114615892], y=[232965], train_mask=[232965], val_mask=[232965], test_mask=[232965])\n",
            "tensor([ 1.2334e+00,  9.0430e+00, -9.2328e-01,  1.0542e+00, -1.1125e+00,\n",
            "        -2.0630e-02,  4.2540e-02,  2.1520e+00, -9.0830e-01,  7.1384e-01,\n",
            "         1.1512e+00, -1.2357e+00,  1.1699e+00, -1.2657e+00, -8.6435e-01,\n",
            "         3.5655e-01, -1.0509e+00,  1.3829e-01, -2.8985e-01, -4.5366e-01,\n",
            "        -1.7766e-01,  7.8746e-01, -1.2298e+00, -9.3718e-01, -2.8484e-02,\n",
            "        -6.2839e-01, -1.5315e-01,  4.0115e-02,  8.6607e-01, -3.5893e-01,\n",
            "        -9.7886e-02,  1.0300e+00, -1.5091e-01, -1.1921e+00,  7.9493e-01,\n",
            "         3.7253e-01,  1.7400e-01, -2.5684e-01, -1.2256e+00, -3.7878e-01,\n",
            "        -1.3944e+00,  4.4133e-01,  8.8975e-01,  7.1985e-02,  1.5196e+00,\n",
            "         1.2643e+00,  3.3659e-01, -8.5132e-01,  1.6793e+00, -5.1868e-01,\n",
            "         1.9735e+00,  1.2463e+00,  4.9956e-01, -7.1537e-01, -3.5420e-01,\n",
            "        -1.8007e+00,  2.3424e+00, -7.6472e-01,  6.8058e-01,  4.0231e-01,\n",
            "        -9.0589e-01,  9.3627e-01, -3.0335e-01,  1.0683e+00,  1.2881e-01,\n",
            "        -5.6879e-01, -7.4037e-01,  1.5159e+00,  1.0466e+00,  1.7760e-01,\n",
            "        -9.1254e-01, -1.7512e+00,  1.7875e+00, -1.9505e+00, -4.8111e-01,\n",
            "        -1.1397e+00, -1.1899e+00, -1.0527e+00, -8.5510e-01, -1.0419e+00,\n",
            "         1.0802e-01,  1.5286e+00, -6.5772e-01,  8.6528e-01, -6.2137e-01,\n",
            "         5.4917e-01,  4.9398e-01, -1.2264e+00, -6.3922e-01,  1.1524e+00,\n",
            "        -1.9678e+00,  6.0463e-02, -8.6241e-01,  7.4588e-01,  4.2979e-02,\n",
            "        -7.3954e-01,  1.4741e-01, -1.1743e+00, -2.1698e+00, -3.1069e-01,\n",
            "         2.1599e-01,  9.8441e-01,  1.6151e+00,  6.4081e-02, -1.5794e+00,\n",
            "         6.8943e-02,  6.7528e-02, -1.2368e+00,  6.0621e-02, -2.2357e-01,\n",
            "        -1.9731e-01, -2.4626e-01, -2.2136e+00,  5.3190e-01, -9.3686e-02,\n",
            "         2.2707e-01,  9.4695e-01,  5.0727e-01, -5.9538e-01,  6.0741e-01,\n",
            "         1.1594e-01, -1.3993e-01,  1.4760e+00,  1.1545e+00, -7.4223e-01,\n",
            "         1.4250e+00,  5.6467e-01,  6.7062e-01, -9.6072e-03, -1.6325e+00,\n",
            "         3.0974e-01,  3.1587e-01, -5.2206e-01,  6.2683e-01,  6.1819e-01,\n",
            "         1.4628e+00, -7.7524e-01, -5.3577e-01,  9.9955e-01,  3.2396e-01,\n",
            "         8.8431e-01,  1.9922e-01, -2.4632e-01, -2.1868e-01,  9.8067e-01,\n",
            "         1.3921e+00, -4.2881e-01, -1.1508e+00, -6.5283e-01, -2.0516e-02,\n",
            "         5.5681e-01, -4.1716e-01, -7.3670e-01, -2.4297e-01, -8.1764e-01,\n",
            "        -8.3104e-01, -1.1405e+00, -6.8082e-01, -3.0834e+00,  4.6139e-01,\n",
            "         6.6941e-01, -3.7514e-01,  1.7697e+00,  8.0945e-01, -1.9996e+00,\n",
            "         7.3443e-01, -1.1893e+00, -4.9696e-01,  2.0365e+00, -1.5473e-01,\n",
            "        -4.4328e-01,  2.9554e-01,  6.5090e-01,  9.9264e-01,  2.2899e+00,\n",
            "         5.5646e-02,  1.0090e-01,  2.3332e-01, -3.8380e-01, -2.4038e-01,\n",
            "         1.1815e+00,  2.5880e-01, -1.8853e-01,  2.1664e-01, -1.0193e+00,\n",
            "        -7.4628e-01,  1.0596e+00,  1.0365e+00, -4.2985e-01, -8.0069e-01,\n",
            "         1.0218e+00, -2.2530e-01, -1.6540e-01, -6.7303e-01,  3.0451e-01,\n",
            "         5.7735e-02,  7.6097e-01, -2.1305e+00, -1.6236e+00, -1.3381e+00,\n",
            "        -2.4638e-02,  2.5604e-01, -5.1423e-01,  1.0485e+00, -1.8218e+00,\n",
            "         2.2373e-02,  3.6606e-01,  1.0498e+00, -3.2278e-02, -5.7483e-01,\n",
            "         6.9267e-01,  6.2246e-01,  1.2267e+00, -4.2621e-02,  4.8254e-01,\n",
            "        -1.4145e+00,  9.3729e-01, -5.7051e-01,  5.0935e-02,  7.4511e-02,\n",
            "         7.9456e-01,  2.5511e-01, -1.3402e+00,  1.1370e+00,  2.2040e+00,\n",
            "        -7.1113e-01,  2.0526e-01,  8.9539e-03,  5.0207e-01,  1.4045e+00,\n",
            "         8.0956e-01, -3.6843e-01,  6.8068e-02, -1.5964e+00,  1.2495e+00,\n",
            "         2.9201e-01,  1.5581e-01, -6.6896e-01, -1.5250e+00,  1.9811e+00,\n",
            "        -5.6403e-01,  2.5356e+00, -8.2312e-01, -6.0688e-01, -4.2799e-02,\n",
            "         2.2765e+00,  7.1216e-01, -1.4135e+00,  3.2234e-01, -1.1826e+00,\n",
            "         5.5348e-01, -1.0122e+00, -3.1750e-01,  1.7088e+00, -5.5218e-02,\n",
            "         2.6533e+00, -5.9774e-01, -1.0538e+00, -9.8970e-01, -1.9714e+00,\n",
            "         1.2117e+00, -6.4787e-01, -1.2258e+00, -3.2500e-01, -1.0704e+00,\n",
            "         1.5060e+00,  7.4217e-01,  1.0795e+00, -2.5709e-01,  7.7096e-01,\n",
            "        -1.8268e+00, -3.9097e-02,  4.2290e-01,  6.8801e-01, -2.5611e+00,\n",
            "        -1.4834e+00,  1.2684e-01,  4.9590e-01,  1.7948e-01, -1.2088e+00,\n",
            "        -6.4251e-01,  6.8626e-01, -1.4066e+00, -6.4155e-01, -6.6542e-01,\n",
            "         8.2215e-01, -1.0691e-01, -2.0264e+00,  6.0394e-01, -1.2515e+00,\n",
            "        -7.3817e-01, -1.4066e+00,  1.1282e+00, -1.7126e-01, -2.1460e+00,\n",
            "         1.4071e-02,  6.9204e-01, -5.5363e-01, -1.3223e+00, -2.0335e+00,\n",
            "        -3.0630e-01, -1.5127e+00, -3.2246e-01,  8.3096e-03, -2.3893e-01,\n",
            "        -9.6422e-02, -7.7393e-02,  1.1951e+00,  1.6875e-01,  6.3016e-01,\n",
            "         3.8970e-01, -2.1069e-01,  1.0078e-01, -1.7433e-01, -1.4962e-01,\n",
            "         4.7277e-01, -2.6743e-01, -1.6074e-01, -3.3780e-01,  1.5102e-01,\n",
            "         6.4623e-02,  6.0822e-01, -4.2829e-01, -3.5606e-01, -1.5443e-01,\n",
            "         1.4409e-01, -2.3891e-01, -2.0926e-01, -3.1680e-01, -2.0371e-01,\n",
            "         5.1748e-01,  7.8114e-02, -7.1335e-02, -6.9934e-01,  4.2598e-01,\n",
            "        -2.3410e-01,  4.7891e-01, -1.3253e-01, -3.5024e-01,  5.3837e-01,\n",
            "        -1.3496e-01,  2.9072e-01,  9.4746e-02, -2.7996e-01, -5.4000e-01,\n",
            "         4.0334e-01,  5.7917e-01, -1.7060e-01,  4.4242e-01, -8.0667e-02,\n",
            "         6.5191e-01,  5.3597e-01, -2.5585e-01, -1.5064e-02,  2.0296e-02,\n",
            "        -1.8323e-01,  2.7899e-01,  3.3544e-01,  5.5523e-01, -2.8464e-02,\n",
            "        -1.2336e-01,  4.3127e-02,  3.9373e-01,  2.3112e-01,  1.6938e-01,\n",
            "         2.6256e-01,  1.0191e-01,  5.9168e-01, -1.2101e-01,  6.6727e-02,\n",
            "        -2.4703e-01, -8.1760e-02,  2.4929e-01, -3.1664e-01, -3.1176e-02,\n",
            "        -4.7980e-01, -2.3597e-01, -4.6935e-01,  5.0637e-01, -5.8147e-01,\n",
            "         4.0602e-01,  7.3600e-01, -6.9142e-01,  1.2796e-01,  5.4467e-02,\n",
            "         3.0549e-01, -6.0495e-03, -2.4872e-01, -6.6004e-01,  2.6585e-01,\n",
            "        -2.1785e-01, -2.5207e-01, -3.8851e-01, -7.9019e-02, -1.1730e-02,\n",
            "         1.4771e-02, -6.1256e-02, -3.3443e-01, -9.3625e-01,  1.0289e-01,\n",
            "        -5.0418e-02,  6.9300e-01,  5.4742e-01, -1.1785e-01,  3.8481e-01,\n",
            "        -1.8526e-02,  2.0177e-01,  6.4617e-02, -6.6790e-01, -5.0027e-01,\n",
            "         9.2940e-04, -2.9096e-01, -5.7768e-01,  4.5282e-02, -1.5304e-01,\n",
            "        -3.3516e-01,  5.4608e-01,  7.4060e-03,  3.0103e-01,  4.8034e-02,\n",
            "         1.2856e-04,  8.0909e-02,  2.4367e-01,  2.9767e-01, -3.2058e-01,\n",
            "        -5.0791e-01,  3.7799e-01, -4.5215e-01, -2.9051e-01, -4.9382e-01,\n",
            "        -1.2667e-01, -3.8234e-01, -3.1983e-01,  3.8636e-01, -8.3775e-02,\n",
            "        -3.4956e-02, -2.0144e-01, -2.8121e-01,  1.9354e-01,  6.6853e-02,\n",
            "        -2.6476e-01, -1.3796e-01,  3.0358e-02,  3.7896e-02,  3.7132e-01,\n",
            "         7.5800e-03,  3.3060e-01, -2.1742e-01, -6.4259e-01, -3.6926e-01,\n",
            "        -5.8650e-01, -4.3248e-01, -7.5828e-01, -5.9038e-01,  3.1906e-03,\n",
            "        -5.2401e-01, -2.2471e-01,  8.0942e-02, -5.6109e-01, -3.3407e-01,\n",
            "         7.0065e-01, -2.5802e-01,  3.1953e-01, -1.5367e-02, -5.2850e-01,\n",
            "         3.5601e-01, -1.8022e-01,  1.0270e-01,  5.6982e-01, -1.2170e-01,\n",
            "         6.4353e-02,  1.2295e-01,  2.4803e-02,  2.2509e-01,  1.0336e-01,\n",
            "        -6.6621e-02, -4.7055e-01,  1.0576e-01, -1.8265e-01,  2.4326e-01,\n",
            "         4.4994e-01, -3.5404e-01, -1.7300e-01, -2.8146e-01, -4.5673e-01,\n",
            "        -5.9643e-01, -2.2017e-01,  3.3448e-01,  6.8420e-03, -1.2102e-01,\n",
            "         4.8495e-01, -2.7838e-01, -1.2583e-01, -1.2770e-01, -1.6386e-01,\n",
            "        -1.1915e-01,  4.7737e-03,  3.8265e-02,  8.2298e-02, -1.7199e-01,\n",
            "        -2.2899e-01,  2.5287e-01, -2.1983e-01,  1.2537e-01, -4.5938e-01,\n",
            "         1.0050e-02, -1.0695e-02, -3.1829e-01, -3.2133e-01, -1.4737e-02,\n",
            "         6.4165e-02, -5.9513e-02,  2.1336e-02,  2.4851e-01, -1.5703e-01,\n",
            "        -5.3545e-01,  7.0085e-02, -1.0958e-01, -9.7016e-02, -2.8047e-01,\n",
            "        -6.0056e-02,  3.8799e-01, -3.9305e-01, -1.0714e-01,  5.0624e-01,\n",
            "        -2.5460e-01,  1.5128e-02, -3.0597e-01,  1.4563e-01, -2.5031e-01,\n",
            "         3.9138e-01, -4.8108e-01, -2.4487e-02,  8.5204e-02,  4.0725e-01,\n",
            "        -2.8677e-01, -3.0267e-01,  1.4580e-01, -1.4783e-01,  1.0023e-01,\n",
            "        -4.6543e-01,  2.3180e-01, -3.8644e-01, -3.3974e-01,  2.2652e-01,\n",
            "         5.5903e-01,  4.0211e-01, -2.5297e-02,  2.8459e-01, -2.7025e-02,\n",
            "         2.7179e-01, -4.9960e-01,  1.3928e-01,  2.5701e-01, -2.7723e-01,\n",
            "         5.7121e-01, -5.5508e-01, -2.1671e-03, -2.5494e-01, -7.4857e-01,\n",
            "         9.7271e-02,  4.8290e-02, -3.9809e-01,  4.5642e-01, -9.7520e-01,\n",
            "        -1.7284e-01,  8.2096e-02,  4.6109e-01,  1.4916e-01, -2.9351e-02,\n",
            "        -7.2795e-01,  3.0064e-01, -2.8890e-01,  4.6227e-01, -5.8538e-01,\n",
            "        -5.0020e-03,  1.0303e-01, -3.7983e-01, -1.2562e-01, -2.5202e-01,\n",
            "        -3.1659e-02,  1.4164e-01, -7.2753e-02, -3.4904e-01, -2.2500e-01,\n",
            "         7.0661e-02,  1.3823e-01,  2.1146e-01,  2.7500e-01, -5.2777e-01,\n",
            "         6.4908e-02, -3.2578e-01,  4.1360e-01, -4.5790e-01,  2.2767e-03,\n",
            "        -1.7817e-01,  9.5668e-02, -4.9747e-01, -4.4391e-01, -2.5790e-01,\n",
            "         3.1119e-01, -3.7721e-01])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Visualize the GNN networks ===================================================================\n",
        "# ## Import packages required for visualization\n",
        "# import matplotlib.pyplot as plt\n",
        "# import networkx as nx\n",
        "# from torch_geometric.utils import to_networkx\n",
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "# def visualize_graph(G, color, ax=None):\n",
        "#     if ax is None:\n",
        "#         ax = plt.gca()\n",
        "#     ax.set_xticks([])\n",
        "#     ax.set_yticks([])\n",
        "\n",
        "#     # plt.sca(ax)\n",
        "#     nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "#                      node_color=color, cmap=\"Set2\")\n",
        "\n",
        "# # plt.cla()\n",
        "# # plt.clf()\n",
        "\n",
        "# ## plt.subplot -> stated-based\n",
        "# ## plt.subplots -> objected-oriented\n",
        "\n",
        "# ## 1st method\n",
        "# # fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
        "\n",
        "# ## 2nd method\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# G = to_networkx(data, to_undirected=True)\n",
        "# visualize_graph(G, color=data.y)\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# ## Unable to plot due to lack of RAM"
      ],
      "metadata": {
        "id": "DNBitEAxHesO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install pyg-lib -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html         # Not working\n",
        "! pip install pyg-lib -f https://data.pyg.org/whl/torch-${TORCH}.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTT7pMB0lDfA",
        "outputId": "4002d5c5-ddda-4b14-c122-8ea3ed6fb118"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: pyg-lib in /usr/local/lib/python3.10/dist-packages (0.4.0+pt24cu121)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_data = sum(data.train_mask)\n",
        "n_val_data = sum(data.val_mask)\n",
        "n_test_data = sum(data.test_mask)\n",
        "\n",
        "print(f'Number of train data : {n_train_data}')\n",
        "print(f'Number of validation data : {n_val_data}')\n",
        "print(f'Number of test data : {n_test_data}')\n",
        "print(f'Number of data : {n_train_data + n_val_data + n_test_data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HU0Cz7Ppzel",
        "outputId": "7f767a2c-5070-4176-f99f-aeb3626ea595"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train data : 153431\n",
            "Number of validation data : 23831\n",
            "Number of test data : 55703\n",
            "Number of data : 232965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mini-batch loader =====================================================================\n",
        "loader = LinkNeighborLoader(data, num_neighbors=[25, 10], batch_size=128)"
      ],
      "metadata": {
        "id": "El9HiTkhHepP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your GNN model ========================================================================\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        # Choose between different GNN building blocks:\n",
        "        self.encoder = GraphSAGE(data.num_features, 16, num_layers=2)\n",
        "        self.decoder = InnerProductDecoder()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_label_index):\n",
        "        x = self.encoder(x, edge_index)\n",
        "        return self.decoder(x, edge_label_index)\n",
        "\n",
        "num_classes = data.y.max().item() + 1\n",
        "model = GNN().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "CvvtZh5PHeme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0600bef-5c7e-432c-c334-c5b491eb9a69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (encoder): GraphSAGE(602, 16, num_layers=2)\n",
            "  (decoder): InnerProductDecoder()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your GNN model =========================================================================\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def train(model, train_loader, mask, n_epochs):\n",
        "    model = model.to(device=device)\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    train_loss_hist, train_acc_hist = [], []\n",
        "    val_loss_hist, val_acc_hist = [], []\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs + 1):\n",
        "        train_loss, train_acc = 0, 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            optimizer.zero_grad()       # Clear gradients\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)     # Compute the loss solely based on the training nodes\n",
        "            train_loss += loss.item() / len(train_loader.dataset)\n",
        "            train_acc += torch.sum(out.argmax(dim=1) == data.y) / len(train_loader.dataset)\n",
        "            loss.backward()     # Derive gradients\n",
        "            optimizer.step()     # Update parameters based on gradients\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            train_loss_hist.append(train_loss)\n",
        "            train_acc_hist.append(train_acc)\n",
        "\n",
        "            val_loss, val_acc = test(model, val_loader)\n",
        "            val_loss_hist.append(val_loss)\n",
        "            val_acc_hist.append(val_acc)\n",
        "            print(f'Epoch : {epoch:03d}, | Train loss : {train_loss:.4f}, | Train accuracy : {train_acc:.4f} | Valid loss : {val_loss:.4f}, | Valid accuracy : {val_acc:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model\n",
        "\n",
        "    return best_model, train_loss_hist, train_acc_hist, val_loss_hist, val_acc_hist\n",
        "\n",
        "def test(model, test_loader, mask):\n",
        "    loss, correct = 0, 0\n",
        "\n",
        "    model.eval()\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x[mask], data.edge_index, data.edge_label_index)\n",
        "        loss += criterion(out[mask], data.y[mask]).item() / len(loader)\n",
        "        correct += torch.sum(out.argmax(dim=1)) == data.y[mask]\n",
        "\n",
        "    acc = correct / len(loader.dataset)\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "kX4v2gqqHejV"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}